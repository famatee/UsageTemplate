{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\n",
    "    'data/assist09/raw/skill_builder_data_corrected.csv',\n",
    "    usecols=['order_id', 'user_id', 'problem_id', 'skill_id', 'correct']\n",
    ").dropna(subset=['skill_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of problems: 17751\n"
     ]
    }
   ],
   "source": [
    "# 建立练习映射,编号从1~n\n",
    "raw_problem=data.problem_id.unique().tolist()\n",
    "raw_problem.sort()\n",
    "num_problem=len(raw_problem)\n",
    "problems={p:i+1 for i,p in enumerate(raw_problem)}\n",
    "print(\"number of problems: %d\" % num_problem)\n",
    "np.save('data/assist09/map/eMap.npy',problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将problem_id替换成eMap的value\n",
    "problems=np.load('data/assist09/map/eMap.npy',allow_pickle=True).item()\n",
    "data=data.replace({\"problem_id\":problems})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of skills: 123\n"
     ]
    }
   ],
   "source": [
    "# 建立技能映射，编号从0~n-1\n",
    "raw_question = data.skill_id.unique().tolist()\n",
    "num_skill = len(raw_question)\n",
    "\n",
    "skills = { p: i for i, p in enumerate(raw_question) }\n",
    "print(\"number of skills: %d\" % num_skill)\n",
    "np.save('data/assist09/map/cMap.npy',skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将skill_id替换成cMap的value\n",
    "skills=np.load('data/assist09/map/cMap.npy',allow_pickle=True).item()\n",
    "data=data.replace({\"skill_id\":skills})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        problem_id  skill_id\n",
      "146242           1      39.0\n",
      "191473           2      49.0\n",
      "112391           3      31.0\n",
      "191472           4      49.0\n",
      "112390           4      31.0\n",
      "...            ...       ...\n",
      "253931       17747      81.0\n",
      "253932       17748      81.0\n",
      "253942       17749      81.0\n",
      "253933       17750      81.0\n",
      "72043        17751      20.0\n",
      "\n",
      "[21246 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "# 建立练习-技能-邻接矩阵\n",
    "adj_problem_skill=np.zeros((num_problem+1,num_skill))\n",
    "single_problem_skill_pair=data.drop_duplicates(subset=['problem_id','skill_id'])[['problem_id','skill_id']].sort_values(by=['problem_id'])\n",
    "print(single_problem_skill_pair)\n",
    "for i,row in single_problem_skill_pair.iterrows():\n",
    "    # adj_problem_skill[problems[row['problem_id']]][skills[row['skill_id']]]=1\n",
    "    adj_problem_skill[int(row['problem_id'])][int(row['skill_id'])]=1\n",
    "#保存e2c邻接矩阵\n",
    "np.save('data/assist09/adj/e2cAdj.npy',adj_problem_skill)\n",
    "#保存e2c-归一化的邻接矩阵\n",
    "norm_e2c=normalize(adj_problem_skill,norm='l1',axis=1)\n",
    "np.save('data/assist09/adj/e2cAdjNorm.npy',norm_e2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        order_id  user_id  problem_id  correct  skill_id\n",
      "0       33022537    64525        1069        1       0.0\n",
      "1       33022709    64525        1080        1       0.0\n",
      "2       35450204    70363        1089        0       0.0\n",
      "3       35450295    70363        1040        1       0.0\n",
      "4       35450311    70363        1126        0       0.0\n",
      "...          ...      ...         ...      ...       ...\n",
      "337996  33150408    85730       15444        0     122.0\n",
      "337997  33150487    85730       15458        1     122.0\n",
      "337998  33150779    85730       15448        1     122.0\n",
      "337999  33151098    85730       15459        0     122.0\n",
      "338000  31950415    87896       15415        0     122.0\n",
      "\n",
      "[283105 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parse student sequence:\t: 100%|██████████| 4163/4163 [00:03<00:00, 1236.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总的学生人数： 4163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 将每个学生的答题序列分好块\n",
    "def parse_all_seq(students):\n",
    "    all_sequences = []\n",
    "    for student_id in tqdm.tqdm(students, 'parse student sequence:\\t'):\n",
    "        student_sequence = parse_student_seq(data[data.user_id == student_id])\n",
    "        all_sequences.extend([student_sequence])\n",
    "    return all_sequences\n",
    "\n",
    "\n",
    "def parse_student_seq(student):\n",
    "    seq = student.sort_values('order_id')\n",
    "    return seq['problem_id'].values,seq['correct'].values\n",
    "\n",
    "data=data.drop_duplicates(subset=['order_id'])\n",
    "print(data)\n",
    "sequences = parse_all_seq(sorted(data.user_id.unique()))\n",
    "print('总的学生人数：',len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<class 'list'>\n",
      "(array([12668, 12692, 12685, 12704, 12705, 12700, 12708,  2993,  3182,\n",
      "        2977,  3173,  3168, 12032, 12242, 12231, 11732, 12213, 11712,\n",
      "       11715]), array([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(type(sequences[0]))\n",
    "print(type(sequences))\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1745832/3321072165.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  allFoldSeq.append((np.array(sequences)[train_index], np.array(sequences)[test_index]))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10,shuffle=True)\n",
    "allFoldSeq=[]\n",
    "\n",
    "for train_index,test_index in kf.split(sequences):\n",
    "    allFoldSeq.append((np.array(sequences)[train_index], np.array(sequences)[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hejunliang/.conda/envs/hjlTorch1/lib/python3.8/site-packages/numpy/lib/npyio.py:521: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    }
   ],
   "source": [
    "np.save('data/assist09/raw/allFoldSeq.npy',allFoldSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_problem_skill=np.load('data/assist09/adj/e2cAdj.npy')\n",
    "def splitToMaxStep(sequences,maxstep):\n",
    "    e_data=[]\n",
    "    a_data=[]\n",
    "    for e_features,a in tqdm.tqdm(sequences, 'splitting into MaxStep: '):\n",
    "        \n",
    "        length=e_features.shape[0]\n",
    "        slices = length//maxstep + (1 if length%maxstep > 0 else 0)\n",
    "        for i in range(slices):\n",
    "            e_temp = np.zeros(shape=[maxstep,1])\n",
    "            a_temp = np.zeros(shape=[maxstep,1])\n",
    "            if length>0:\n",
    "                if length>=maxstep:\n",
    "                    l=maxstep\n",
    "                else:\n",
    "                    l=length\n",
    "                for j in range(l):\n",
    "                    e_temp[j]=e_features[i*maxstep+j]\n",
    "                    a_temp[j]=a[i*maxstep+j]\n",
    "                length = length - maxstep\n",
    "            e_data.append(e_temp)\n",
    "            a_data.append(a_temp)\n",
    "    \n",
    "    return np.concatenate((np.array(e_data).astype(float),np.array(a_data).astype(float)),axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "splitting into MaxStep: 100%|██████████| 3746/3746 [00:00<00:00, 16641.97it/s]\n",
      "splitting into MaxStep: 100%|██████████| 417/417 [00:00<00:00, 18644.53it/s]\n",
      "splitting into MaxStep: 100%|██████████| 3746/3746 [00:00<00:00, 17314.17it/s]\n",
      "splitting into MaxStep: 100%|██████████| 417/417 [00:00<00:00, 14834.31it/s]\n",
      "splitting into MaxStep: 100%|██████████| 3746/3746 [00:00<00:00, 16719.02it/s]\n",
      "splitting into MaxStep: 100%|██████████| 417/417 [00:00<00:00, 16959.09it/s]\n",
      "splitting into MaxStep: 100%|██████████| 3747/3747 [00:00<00:00, 16911.92it/s]\n",
      "splitting into MaxStep: 100%|██████████| 416/416 [00:00<00:00, 15344.03it/s]\n",
      "splitting into MaxStep: 100%|██████████| 3747/3747 [00:00<00:00, 16944.92it/s]\n",
      "splitting into MaxStep: 100%|██████████| 416/416 [00:00<00:00, 18391.42it/s]\n",
      "splitting into MaxStep: 100%|██████████| 3747/3747 [00:00<00:00, 16790.05it/s]\n",
      "splitting into MaxStep: 100%|██████████| 416/416 [00:00<00:00, 14598.53it/s]\n",
      "splitting into MaxStep: 100%|██████████| 3747/3747 [00:00<00:00, 16954.00it/s]\n",
      "splitting into MaxStep: 100%|██████████| 416/416 [00:00<00:00, 15106.23it/s]\n",
      "splitting into MaxStep: 100%|██████████| 3747/3747 [00:00<00:00, 16659.62it/s]\n",
      "splitting into MaxStep: 100%|██████████| 416/416 [00:00<00:00, 17786.42it/s]\n",
      "splitting into MaxStep: 100%|██████████| 3747/3747 [00:00<00:00, 16955.43it/s]\n",
      "splitting into MaxStep: 100%|██████████| 416/416 [00:00<00:00, 15124.04it/s]\n",
      "splitting into MaxStep: 100%|██████████| 3747/3747 [00:00<00:00, 16823.89it/s]\n",
      "splitting into MaxStep: 100%|██████████| 416/416 [00:00<00:00, 16223.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import _pickle as pickle\n",
    "allFoldSeq=np.load('data/assist09/raw/allFoldSeq.npy',allow_pickle=True)\n",
    "MAX_STEP = 128\n",
    "i=1\n",
    "for train_sequences,test_sequences in allFoldSeq:\n",
    "    train_data=splitToMaxStep(train_sequences,MAX_STEP)\n",
    "    test_data=splitToMaxStep(test_sequences,MAX_STEP)\n",
    "    \n",
    "    trainFile=open('data/assist09/raw/train/train_data'+'_'+str(i)+'.txt','wb')\n",
    "    testFile=open('data/assist09/raw/test/test_data'+'_'+str(i)+'.txt','wb')\n",
    "    pickle.dump(train_data,trainFile)\n",
    "    pickle.dump(test_data,testFile)\n",
    "    trainFile.close()\n",
    "    testFile.close()\n",
    "    i=i+1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hjlTorch1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "290091e372d03c8158b7bbd399fcd5d922307b87125be2eaecd4834ca6d9de07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
